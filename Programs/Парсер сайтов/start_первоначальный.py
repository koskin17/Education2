'''
Для получения версии pip в командной строке нужно указать:
pip -V

Для получения информации про установленные модули в командной
строке нужно указать:
pip freeze

Для парсинга веб-страниц нужно установить пакет beautifulsoup.
В данном случае устанавливается версия 4.
Проверять версию пакета нужно на сайте pypi.org
'''

'''
После установки модуля его нужно импортировать
'''
from bs4 import BeautifulSoup

'''
Также нужен модуль urllib.reduest.
Он встроен в ядро Python и его нужно просто импортировать
'''
import urllib.request

'''
Для начала парсинга нужно сделать запрос
и передат url того сайта, который мы хотим спарсить.
'''
request = urllib.request.urlopen('https://www.ua-football.com/sport')
##print(request) # здесь мы проверяем результат и раз получен объект запроса - ошибок нет
html = request.read()
'''
Для использования модуля BeautifulSoup нужно создать объект
и параметром передать объект, который мы прочитали в html.
При это рекомендуется 
'''

soup = BeautifulSoup(html, 'html.parser')

'''
Для получения только новостей:
- создаётся переменная News и используется метод find_all.
При этом нужно изучить страницу, которую парсим (парсинг всегда индивидуален для каждого сайта)
и понять, как, из чего состоят новости.
В данном случае новости представлены на сайте элементами списка ul
класса liga-news-item. Следовательно их все нужно найти.
Делается это передачей в find_all элементы 'li' с классом (class) liga-news-item
Класс обязательно описывается (подписывается) с символом подёркивания - class_
'''
news = soup.find_all('li', class_ = 'liga-news-item')
'''
В результате выводится список именно новостей без прочего html-кода
'''
##print(news)
'''
Раз мы получили список новостей, значит в нём есть элементы и
значит по нему можно пройтись циклом for
В данном случае нам нужно:
- название новости;
- краткое описание новости;
- ссылка на новость.

Для хранения этих данных создаём переменную results и это будет список,
внутрь которого помешаются словари.
Словари - потому что намного лучше и проще использовать "говорящие" ключи,
под которыми будет понятно что хранится.

Анализируем html-код сраницы и выделяем уникальный элемент
в классах, которые присовены title, description и href каждой новости.

'''
results = []

for item in news:
    ''' В данном случае определено, что в title есть d-block.
    Таким образом для title мы ищем d-block в классе span.
    Для того, чтобы не получать "обёртку" span применяется метод
    get_text().
    Для обрезки лишних пробелов применяется метод strip.
    Аналогичный метод есть в качестве параметра для метода get_text - параметр strip,
    который должен быть равен True.
    '''
    title = item.find('span', class_ = 'd-block').get_text(strip = True)

    '''
    Аналогичным образом получается краткое описание новости - description.
    На странице, которая парсится, ищется класс с именем name-dop.
    '''
    desc = item.find('span', class_ = 'name-dop').get_text(strip = True)

    '''
    Аналогичным образом получается ссылка на новость - href.
    В данном случае ссылкой является весь блок item.
    Для этого просто указываем item.a (найти тег а) и методом get получаем href.
    '''
    href = item.a.get('href')

    '''
    Теперь, когда все три необходимые характеристики получены,
    их надо добавить в словарь
    '''
    results.append({
        'title': title,
        'desc': desc,
        'href': href})

'''
Теперь записываем весь полученный словарь в файл.
'''
file= open('news.txt', 'w', encoding = 'utf-8')

'''
Создаём счётчик для нумерации новостей
'''
i = 1

for item in results:
    file.write(f'Новость № {i}\nЗаголовок новости: {item["title"]}\n')
    file.write(f'Описание новости: {item["desc"]}\n')
    file.write(f'Ссылка на новость: {item["href"]}\n')
    file.write('\n*****************************************\n')
    i += 1

file.close()
